---
title: "TMA4250-Project-3"
author: "Ole Riddervold, Ole Kristian Skogly"
date: "2023-04-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("functions.R")

library(ggplot2)
library(patchwork)
library(tidyverse)
library(rgdal)
library(spdep)
library(latex2exp)
library(LaplacesDemon)
library(pracma)
library(MASS)
library(reshape2)
library(igraph)
```

# Geography data

The data is loaded into the R environment as **nigeriaAdm1** and **nigeriaAdm2** respectively.
```{r}
load("data/Admin1Geography.RData")
load("data/Admin2Geography.RData")
```


# Problem 1

## a)

## b)

## c)

## d)


# Problem 2

## a)

```{r}
direct_estimates <- read.table(
  "data/DirectEstimates.txt",
  skip=1,
  col.names=c("area_name", "obs", "stdev")
)


plotAreaCol(
  "figures/2a.jpg",
  width=25,
  height=20,
  estVal=invlogit(direct_estimates$obs),
  geoMap=nigeriaAdm1,
  leg=TeX("$\\hat{p}$")
)
```
It is clear that there is a considerable amount of dependence between the various regions, and therefore borrowing strength in space to reduce uncertainty is indeed reasonable.


## b)
### Calculating the parameters for posterior distribution
```{r}
# Treating data:
y <- direct_estimates$obs
D <- diag(direct_estimates$stdev)
D.inv <- solve(D)
n <- length(y)
sigma_squared <- 100^2

# Posterior distribution parameters:
simple.Sigma <- solve( D.inv + (1/sigma_squared)*identity(n) )
simple.mu    <- simple.Sigma %*% D.inv %*% y
```

### Empirical estimation of median and standard deviation 
We wish to do an empirical estimation of the median and standard deviation of $P_a | Y_a=y_a, \: a\in \{1, \cdots, n\}$
```{r}
realization_summary <- function(realizations) {
  # Given a (kxn) array of realizations for many observations,
  # calculates the meadian and standard deviation for each realization.
  # Args:
  #   realizations ((kxn) array): Array of k realizations of n variables
  # Returns:
  #   Dataframe with n rows containing median and std.
  return(
    realizations %>%
      melt(varnames=c("sample", "a")) %>%
      group_by(a) %>%
      summarise(median.est=median(value), std.est=std(value))
  )
}
```

```{r}
simple.samples <- sigmoid(mvrnorm(simple.mu, simple.Sigma, n=100))
simple.sum <- realization_summary(simple.samples)
```
Plotting:
```{r}
plotAreaCol(
  "figures/2b_median.jpg",
  width=25,
  height=20,
  estVal=simple.sum$median.est,
  geoMap=nigeriaAdm1,
  leg="median est."
)

plotAreaCol(
  "figures/2b_std.jpg",
  width=25,
  height=20,
  estVal=simple.sum$std.est,
  geoMap=nigeriaAdm1,
  leg=TeX("$\\hat{\\sigma}$")
)
```


## c)
Reading the graph and generating the structure matrix:
```{r}
G <- read.table("data/Admin1Graph.txt") %>% as.matrix()
R <- generate_structure_matrix(G)
```


### Helper function to generate structure matrix:
```{r}
generate_structure_matrix <- function(G) {
  # Generates the structure matrix of a Besag model based
  # on the graph G.
  # Args:
  #   G ((nxn) matrix): Matrix representation of G
  # Returns:
  #   (nxn) structure matrix
  R <- -G
  diag(R) <- colSums(t(G))
  return(R)
}
```

### Execution:
```{r}
evaluate_besag_and_plot <- function(tau, filenames) {
  # Evaluates the besag posterior, samples and summarises, and
  # finally plots the data. Relevant for task c and e.
  # Args:
  #   tau               (float): Besag hyperparameter
  #   filenames (length-2 list): Filenames of plots
  # Returns:
  #   None
  # Posterior distribution parameters:
  besag.Sigma <- solve(tau*R + D.inv)
  besag.mu <- besag.Sigma %*% D.inv %*% y
  
  # Sampling from the distribution
  besag.samples <- sigmoid(mvrnorm(besag.mu, besag.Sigma, n=100))
  besag.sum <- realization_summary(besag.samples)
  
  plotAreaCol(
    filenames[1],
    width=25,
    height=20,
    estVal=besag.sum$median.est,
    geoMap=nigeriaAdm1,
    leg="median est."
  )
  
  plotAreaCol(
    filenames[2],
    width=25,
    height=20,
    estVal=besag.sum$std.est,
    geoMap=nigeriaAdm1,
    leg=TeX("$\\hat{\\sigma}$")
  )
}
```

```{r}
evaluate_besag_and_plot(
  tau=1.0,
  filenames=c("figures/2c_median.jpg", "figures/2c_std.jpg")
)
```

One can immediately observe that the median estimate is more "smooth" across space, and that the estimate of the standard deviation is considerably smaller for the model with a Besag hierarchical model. The estimated standard deviation also seems to be more varying across states, but one has to remember that the color scale makes this look more significant than it is, considering that $\hat{\sigma}$ only ranges from 0.6 to 0.9.

## d)
```{r}
evaluate_updated_besag_and_plot <- function(tau, update, filenames) {
  # Evaluates the updated besag posterior with Kaduna as an additional sample,
  #and summarises, and finally plots the data. Relevant for task c and e.
  # Args:
  #   tau               (float): Besag hyperparameter
  #   update             (list): Observation and variance of update
  #   filenames (length-2 list): Filenames of plots
  # Returns:
  #   None
  # Posterior distribution parameters:
  A <- diag(1, nrow=38, ncol=37)
  A[38, 19] <- 1
  D.tilde.inv <- diag(c(direct_estimates$stdev, update$var)) %>% solve()
  besag.Sigma <- solve(tau*R + t(A)%*%D.tilde.inv%*%A)
  besag.mu <- besag.Sigma %*% t(A) %*% D.tilde.inv %*% y
  
  # Sampling from the distribution
  besag.samples <- sigmoid(mvrnorm(besag.mu, besag.Sigma, n=100))
  besag.sum <- realization_summary(besag.samples)
  
  plotAreaCol(
    filenames[1],
    width=25,
    height=20,
    estVal=besag.sum$median.est,
    geoMap=nigeriaAdm1,
    leg="median est."
  )
  
  plotAreaCol(
    filenames[2],
    width=25,
    height=20,
    estVal=besag.sum$std.est,
    geoMap=nigeriaAdm1,
    leg=TeX("$\\hat{\\sigma}$")
  )
}
```

```{r}
A <- diag(1, nrow=38, ncol=37)
A[38, 19] <- 1
D.tilde.inv <- diag(c(direct_estimates$stdev, 0.1^2)) %>% solve()
besag.Sigma <- tau*R + t(A)%*%D.tilde.inv%*%A
```
TODO: NEED TO CHANGE R AS WELL


```{r}
evaluate_updated_besag_and_plot(
  tau=1.0,
  update=0.5,
  filenames=c("figures/2d_median.jpg", "figures/2d_std.jpg")
)
```



## e)
$\tau = 0.1$
```{r}
evaluate_besag_and_plot(
  tau=0.1,
  filenames=c("figures/2e_0,1_median.jpg", "figures/2e_0,1_std.jpg")
)
```

$\tau = 10$
```{r}
evaluate_besag_and_plot(
  tau=10.0,
  filenames=c("figures/2e_10_median.jpg", "figures/2e_10_std.jpg")
)
```
Comparing the two new plots with the original one from c), one can observe the following effects of changing $\tau$:
\begin{itemize}
  \item Small $\tau$ decreases the interspatial effect of each node in the graph (each state), and makes the model less spatially dependent. Looking that the posterior distribution, this is quite clear, as $\lim_{\tau \to 0}{\bm{X}|\bm{Y}} \sim \mathcal{N}_n(\bm{y}, \mathbf{D})$, which is the same as the case when $\sigma^2 \to \infty$ in the posterior distribution from Section \ref{sec:2_b}. Smaller $\tau$ also increases the variance of all estimates slightly.
  \item Large $\tau$ provides a large amount of spatial smoothing for all estimates, and makes the model more spatially dependent. Looking at the effect of a large $\tau$ in the $\bm{\mu}_C$ and $\mathbf{Q}_C$, it is clear that the spatial term becomes dominant, neglecting the "independent" term provided by $\mathbf{D}$. In this case, the variance is also reduced, implying that spatial predictions gives a less variant model.
\end{itemize}
It is clear that different $\tau$ can lead to vastly different estimates, and it is therefore very important to estimate $\tau$ correctly.

## f)
```{r}
Q_C <- function(tau) {
  # Precision matrix of a Besag posterior model.
  # Args:
  #   tau (float)
  # Returns:
  #   Q_C ((nxn) matrix)
  return(
    tau*R + D.inv
  )
}

mu_C <- function(Q_C) {
  # Mean of a Besag posterior model.
  # Args:
  #   Q_C ((nxn) matrix): Precision matrix of a besag posterior model
  # Returns:
  #   mu_C (length-n list)
  return(
    Q_C %*% D.inv %*% y
  )
}

log_likelihood <- function(tau) {
  # Log likelihood function of tau given the data y
  # The constant is not included, as this is not important to perform an MLE.
  # Args:
  #   tau (float)
  # Returns:
  #   l(tau; y)
  Q_C <- Q_C(tau)
  mu_C <- mu_C(Q_C)
  return(
    (36/2)*log(tau) - (tau/2)*t(mu_C) %*% R %*% mu_C - (1/2)*t(y-mu_C) %*% D.inv %*% (y-mu_C) -
      (1/2)*log(det(Q_C))
  )
}
```

```{r}
tau_hat <- optimize(log_likelihood, interval=c(0, 10000), maximum=TRUE)
```

```{r}
tau_hat
```

